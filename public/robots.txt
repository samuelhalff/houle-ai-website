# https://www.robotstxt.org/robotstxt.html
# Robots.txt for houle.ai - Swiss Enterprise AI Platform

# ============================================
# Search Engine Crawlers (Allow Full Access)
# ============================================
User-agent: Googlebot
Allow: /

User-agent: Googlebot-Image
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

User-agent: Applebot
Allow: /

User-agent: Yandex
Allow: /

User-agent: Baiduspider
Allow: /

# ============================================
# AI Crawlers (Allow for Indexation Visibility)
# ============================================
# OpenAI's GPTBot - Allow for AI visibility
User-agent: GPTBot
Allow: /

# Common Crawl - Allow for research/training datasets
User-agent: CCBot
Allow: /

# Anthropic AI Crawler
User-agent: anthropic-ai
Allow: /

User-agent: Claude-Web
Allow: /

# Google AI (Gemini/Bard training)
User-agent: Google-Extended
Allow: /

# Perplexity AI
User-agent: PerplexityBot
Allow: /

# Cohere AI
User-agent: cohere-ai
Allow: /

# ============================================
# Default Rule
# ============================================
User-agent: *
Allow: /

# ============================================
# Blocked Routes (Internal/Technical)
# ============================================
# Disallow internal/API routes
Disallow: /api/
Disallow: /_next/

# Block crawl-heavy query parameters to prevent index bloat
Disallow: /*?*

# Block non-canonical URL variations (without trailing slash)
Disallow: /fr$
Disallow: /en$
Disallow: /de$
Disallow: /es$
Disallow: /pt$

# Block draft and preview routes if any
Disallow: /draft/
Disallow: /preview/

# ============================================
# Sitemap Location
# ============================================
Sitemap: https://houle.ai/sitemap.xml
